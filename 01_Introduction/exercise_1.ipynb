{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/antonio-sntos/alg-fair-class/blob/main/01_Introduction/exercise_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jokf37CsGVr-"
      },
      "source": [
        "# Lecture 1: Introduction\n",
        "This notebook is a part of the [Algorithmic Fairness, Accountability and Ethics (Spring 2026)](https://learnit.itu.dk/course/view.php?id=3025445) at [IT-University of Copenhagen](https://itu.dk/)\n",
        "\n",
        "\n",
        "Today you have seen the example of bias in [ImageNet dataset](https://www.image-net.org/). Let's see if we can identify bias using other datasets and models.\n",
        "\n",
        "We are going to work with the [CLIP](https://openai.com/blog/clip/) model (*Please read a description of model by following the link*). A little overview of the model:\n",
        "1. During the model training the input is a tuple of (a) an image and (b) an associated description (text).\n",
        "2. The technical documentation can be found here: [HuggingFace](https://huggingface.co/openai/clip-vit-base-patch16)\n",
        "3. Note that the model weights aound 600MB. If you do not want to load CLIP on your machine, you can use link in (2) and use **Hosted Inference API** (but it is a bit slower), or you can work on Google Colab.\n",
        "4. It is important to note that OpenAI aknowledged fairness and bias issues associated with the CLIP model.\n",
        "5. We are going to see how gender bias manifests itself.\n",
        "\n",
        "The [dataset](https://huggingface.co/datasets/SDbiaseval/identities-sd-1.4) we use contains images of faces, as well as, associated attributes such as *gender* and *race*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiFCSMvyGVr_"
      },
      "source": [
        "## 1. Setup\n",
        "__If you have trouble loading datasets after having installed the package (```pip install datasets```), pleae restart your kernel and try again__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhionLw1GVsA"
      },
      "outputs": [],
      "source": [
        "# Here are the packages that you need to install\n",
        "import pandas as pd\n",
        "from transformers import CLIPProcessor, CLIPModel # pip install transformers\n",
        "from datasets import load_dataset ## pip install datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Gbo-L7GVsA"
      },
      "outputs": [],
      "source": [
        "# 1. Download the dataset\n",
        "dataset = load_dataset(\"SDbiaseval/identities-sd-1.4\")\n",
        "# 2. Download + set up the model and processor (if you want to know more about these, read the Huggingface documentation)\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZbI22H-GVsA"
      },
      "source": [
        "## 2. Making Predictions\n",
        "To access a record/sample you can use the snippet below. Here we access the first image in the dataset. As you can see it is a dict:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mpRqI57GVsA"
      },
      "outputs": [],
      "source": [
        "sample = dataset[\"train\"][0]\n",
        "sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "1SGUlyZ4GVsB"
      },
      "outputs": [],
      "source": [
        "## To view an image you can simply do the following:\n",
        "sample[\"image\"]\n",
        "## To access gender\n",
        "# sample[\"gender\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dy-vub4VGVsB"
      },
      "source": [
        "**CLIP allows us to specify arbitrary categories:**\n",
        "* Along with the input image, we can pass these categories\n",
        "* The model returns a probability that the picture belongs to one of the specified categories.\n",
        "Let's say I want to know whether the image is closer to portrait of an *engeneer* or a *nurse*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X28RgA9cGVsB"
      },
      "outputs": [],
      "source": [
        "## Create an input the the model\n",
        "labels = [\"a photo of an scientist\", \"a photo of a nurse\"]\n",
        "image = sample[\"image\"]\n",
        "inputs = processor(text=labels,\n",
        "                    images=image, return_tensors=\"pt\", padding=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS6fqQrFGVsC"
      },
      "source": [
        "Now that we have appropriate representation of the input, we can ask CLIP to make a prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkq9ev_-GVsC"
      },
      "outputs": [],
      "source": [
        "outputs = model(**inputs)\n",
        "logits_per_image = outputs.logits_per_image # this is the image-text similarity score\n",
        "probs = logits_per_image.softmax(dim=1).detach() # this gives you the probabilities\n",
        "probs = probs.view(-1) # view - flattens the array\n",
        "probs\n",
        "## or you can calculate the label with:\n",
        "# probs.argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sH7V1qpqGVsC"
      },
      "source": [
        "According to CLIP model, the first image in our dataset is closer to the notion of *a photo of a scientist*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtpnTnYdGVsC"
      },
      "source": [
        "## 3. Your Task\n",
        "#### 3.1. Simple auditing of CLIP\n",
        "Now we want to know if there is a difference between predictions given to females and males images (when it comes to the *a photo of a scientist* and *a photo of a nurse* categories). This is where you come in:\n",
        "1. Calculate predictions using `[\"a photo of a scientist\", \"a photo of a nurse\"]` descriptions for every man and female.\n",
        "2. Find **median** (or mean) for the *man* and *woman* groups\n",
        "    * (**optional**) Use your favourite method, e.g. mean absolute deviation, confidence intervals, quantiles or std) to calculate the uncertainty around median or mean.\n",
        "3. Plot the results and describe what you see."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zBqgRvsGVsC"
      },
      "outputs": [],
      "source": [
        "labels = [\"a photo of a scientist\", \"a photo of a nurse\"]\n",
        "male_scores = []\n",
        "female_scores = []\n",
        "\n",
        "for i in range(len(dataset[\"train\"])):\n",
        "    sample = dataset[\"train\"][i]\n",
        "    image = sample[\"image\"]\n",
        "    inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n",
        "    outputs = model(**inputs)\n",
        "    probs = outputs.logits_per_image.softmax(dim=1).detach().cpu().numpy().flatten()\n",
        "    if sample[\"gender\"] == \"man\":\n",
        "        male_scores.append(probs)\n",
        "    elif sample[\"gender\"] == \"woman\":\n",
        "        female_scores.append(probs)\n",
        "\n",
        "male_scores = np.array(male_scores)\n",
        "female_scores = np.array(female_scores)\n",
        "\n",
        "male_median = np.median(male_scores, axis=0)\n",
        "female_median = np.median(female_scores, axis=0)\n",
        "\n",
        "print(f\"Median scores for Men: {dict(zip(labels, male_median))}\")\n",
        "print(f\"Median scores for Women: {dict(zip(labels, female_median))}\")\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x - width/2, male_median, width, label='Man')\n",
        "ax.bar(x + width/2, female_median, width, label='Woman')\n",
        "\n",
        "ax.set_ylabel('Probability')\n",
        "ax.set_title('CLIP Prediction Medians by Gender')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXRarZyyGVsC"
      },
      "source": [
        "#### 3.2. Female vs Male Concepts\n",
        "In the paper [Gender Bias in Word Embeddings:\n",
        "A Comprehensive Analysis of Frequency, Syntax, and Semantics](https://arxiv.org/pdf/2206.03390.pdf), the authors prove that NLP algorithms inherit bias when it comes to gender (i.e. some words become more *male-associated* and some become more *female-associated*). Let's check whether CLIP has similar biases:\n",
        "1. Go to the **Appendix A.1** of the paper mentined above. You will find a list of male-/female- associated words.\n",
        "2. Use any of them to specify your own version of `[\"a photo of a scienntist\", \"a photo of a nurse\"]`. You can come up with any descriptions you want and you can use more description (more than 2).\n",
        "3. Perform Similar analysis as in **Section 3.1**\n",
        "4. Does your example have any difference between *man* and *woman* scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqKGGXvcGVsC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qruvcr3NGVsC"
      },
      "source": [
        "## 3.3. Neutral Concepts\n",
        "Choose descriptions that (in your opinion) are neutral and perform the analysis in **Section 3.1** one more time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vdnll79GVsC"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "AFAE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}